{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41234ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nthadishetty1/frame_interpollation/plucker.py:58: UserWarning: Using torch.cross without specifying the dim arg is deprecated.\n",
      "Please either pass the dim explicitly or simply use torch.linalg.cross.\n",
      "The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at /pytorch/aten/src/ATen/native/Cross.cpp:63.)\n",
      "  rays_dxo = torch.cross(rays_o, rays_d)                          # B, V, HW, 3\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'metadata_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m left_data \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     18\u001b[0m right_data \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 19\u001b[0m left_meta_path \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     20\u001b[0m right_meta_path \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'metadata_path'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append('/home/nthadishetty1/frame_interpollation/CameraCtrl')\n",
    "\n",
    "from CameraCtrl.cameractrl.models.pose_adaptor import CameraPoseEncoder, PoseAdaptor\n",
    "from data import StereoEventDataset\n",
    "lucker_embeddings = StereoEventDataset.plucker_embeddings\n",
    "\n",
    "\n",
    "train_dataset = StereoEventDataset(video_data_dir=\"/home/nthadishetty1/frame_interpollation/stereo_vkitti_folders\",frame_height=375,frame_width=375)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,shuffle=None,collate_fn=None,batch_size=1, num_workers=1)\n",
    "\n",
    "for batch_idx, batch in enumerate(train_dataloader):\n",
    "    if batch_idx >= 3:\n",
    "        break\n",
    "    video_name = batch['video_name'][0]     \n",
    "    left_data = batch['left']\n",
    "    right_data = batch['right']\n",
    "    left_meta_path = batch['left']['metadata_path']\n",
    "    right_meta_path = batch['right']['metadata_path']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee5094b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NpzFile '/data/venkateswara_lab/frame_interpollation/bs_ergb/3_TRAINING/acquarium_02/events/000000.npz' with keys: x, y, timestamp, polarity\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "k =np.load('/data/venkateswara_lab/frame_interpollation/bs_ergb/3_TRAINING/acquarium_02/events/000000.npz')\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eec389ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from natsort import natsorted\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# ------------------- Helper Functions -------------------\n",
    "\n",
    "def mask_function(event_image, kernel_size=31, kernel_size_erode=61,kernel_size_midele=31, iterations=1, sigma_log=10):\n",
    "    max_value = np.max(np.abs(event_image))\n",
    "    if max_value != 0:\n",
    "        event_image = np.abs(event_image) / max_value\n",
    "    else:\n",
    "        event_image = np.abs(event_image)\n",
    "\n",
    "    event_image_blurred = cv2.GaussianBlur(event_image, (kernel_size,kernel_size), sigma_log)\n",
    "    _, binary_image = cv2.threshold(event_image_blurred, 0.01, 1, cv2.THRESH_BINARY)\n",
    "    kernel_dilate = np.ones((kernel_size_erode, kernel_size_erode), np.uint8)\n",
    "    binary_image_dilated = cv2.dilate(binary_image, kernel_dilate, iterations=iterations)\n",
    "    binary_median = cv2.medianBlur(binary_image_dilated.astype(np.uint8), kernel_size_midele)\n",
    "    return binary_median\n",
    "\n",
    "def save_debug_images_as_rgb(debug_dir, event_image, cumulative_image, B):\n",
    "    os.makedirs(debug_dir, exist_ok=True)\n",
    "    for b in range(B):\n",
    "        event_channel = event_image[:, :, b]\n",
    "        cumulative_channel = cumulative_image[:, :, b]\n",
    "        event_channel_rgb = np.clip(event_channel * 255, 0, 255).astype(np.uint8)\n",
    "        cumulative_channel_rgb = np.clip(cumulative_channel * 255, 0, 255).astype(np.uint8)\n",
    "        cv2.imwrite(os.path.join(debug_dir, f\"event_image_channel_{b}.png\"), event_channel_rgb)\n",
    "        cv2.imwrite(os.path.join(debug_dir, f\"cumulative_image_channel_{b}.png\"), cumulative_channel_rgb)\n",
    "\n",
    "def create_event_image(args, x, y, p, t, shape, B=6, debug=False):\n",
    "    height, width = shape[:2]\n",
    "    event_image = np.zeros((height, width, B), dtype=np.float32)\n",
    "    cumulative_image = np.zeros((height, width, B), dtype=np.float32)\n",
    "    \n",
    "    start_time, end_time = t[0], t[-1]\n",
    "    delta_T = end_time - start_time\n",
    "    normalized_timestamps = (B - 1) * (t - start_time) / delta_T\n",
    "    \n",
    "    x = np.clip(x.astype(int), 0, width - 1)\n",
    "    y = np.clip(y.astype(int), 0, height - 1)\n",
    "\n",
    "    bin_idx = np.round(normalized_timestamps).astype(int)\n",
    "    bin_idx = np.clip(bin_idx, 0, B - 1)\n",
    "\n",
    "    weights = np.maximum(0, 1 - np.abs(normalized_timestamps - bin_idx))\n",
    "    np.add.at(event_image, (y, x, bin_idx), p * weights)\n",
    "\n",
    "    norm_value_evs = np.maximum(np.abs(np.min(event_image)), np.max(event_image))\n",
    "    event_image = (event_image + norm_value_evs)/ (2 * norm_value_evs)\n",
    "\n",
    "    if args.event_filter == \"great_filter\":\n",
    "        for i in range(0, B, 3):\n",
    "            mask_group = []\n",
    "            for j in range(i, min(i + 3, B)):\n",
    "                mask = (bin_idx <= j) & (bin_idx >= np.maximum(0, j - 2))\n",
    "                cumulative_image_f = cumulative_image[:, :, j].copy()\n",
    "                np.add.at(cumulative_image_f, (y[mask], x[mask]), np.abs(p[mask]))\n",
    "                motion_mask = mask_function(cumulative_image_f)\n",
    "                mask_group.append(motion_mask)\n",
    "            combined_mask = np.logical_or.reduce(mask_group)\n",
    "            for j in range(i, min(i + 3, B)):\n",
    "                cumulative_image[:, :, j] = combined_mask.astype(np.uint8)\n",
    "        if debug:\n",
    "            save_debug_images_as_rgb(\"./debug_output\", event_image, cumulative_image, B)\n",
    "        return event_image * cumulative_image\n",
    "    else:\n",
    "        return event_image\n",
    "\n",
    "# ------------------- Dataset Class -------------------\n",
    "\n",
    "class StableVideoDataset(Dataset):\n",
    "    def __init__(self, args, video_data_dir, max_num_videos=None, frame_height=576, frame_width=1024, num_frames=14,\n",
    "                 is_reverse_video=True, random_seed=42, skip_sampling_rate=1):\n",
    "        \n",
    "        self.video_data_dir = video_data_dir\n",
    "        video_names = sorted([video for video in os.listdir(video_data_dir) \n",
    "                              if os.path.isdir(os.path.join(video_data_dir, video))])\n",
    "        self.length = min(len(video_names), max_num_videos) if max_num_videos else len(video_names)\n",
    "        self.video_names = video_names[:self.length]\n",
    "\n",
    "        self.skip_sampling_rate = skip_sampling_rate\n",
    "        if skip_sampling_rate < 1:\n",
    "            self.skip_sampling_rate = random.choices([1, 2, 3], weights=[0.5, 0.3, 0.2], k=1)[0]\n",
    "\n",
    "        self.sample_frames = num_frames * self.skip_sampling_rate + 1 - self.skip_sampling_rate\n",
    "        self.sample_stride = self.skip_sampling_rate\n",
    "        print(\"skip/sample_frames/stride:\", self.skip_sampling_rate, self.sample_frames, self.sample_stride)\n",
    "\n",
    "        self.event_nums_between = num_frames\n",
    "        self.frame_width = frame_width\n",
    "        self.frame_height = frame_height\n",
    "        self.pixel_transforms = transforms.Compose([\n",
    "            transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "        ])\n",
    "        self.is_reverse_video = is_reverse_video\n",
    "        self.args = args\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    def load_npz(self, file_path):\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        return data['x'], data['y'], data['polairty'], data['time']\n",
    "\n",
    "    def accumulate_events(self, video_frame_paths, rgb_frame_paths, B=6):\n",
    "        x_list, y_list, p_list, t_list = [], [], [], []\n",
    "        video_frames = []\n",
    "        shape = None\n",
    "\n",
    "        for event_path, rgb_path in zip(video_frame_paths, rgb_frame_paths):\n",
    "            x, y, p, t = self.load_npz(event_path)\n",
    "            x_list.append(x); y_list.append(y); p_list.append(p); t_list.append(t)\n",
    "\n",
    "            frame = np.array(Image.open(rgb_path)).astype(np.float32)/255.0\n",
    "            video_frames.append(frame)\n",
    "            shape = frame.shape[:2]\n",
    "\n",
    "        x_all = np.concatenate(x_list)\n",
    "        y_all = np.concatenate(y_list)\n",
    "        p_all = np.concatenate(p_list)\n",
    "        t_all = np.concatenate(t_list)\n",
    "\n",
    "        event_voxel_bin = create_event_image(self.args, x_all, y_all, p_all, t_all, shape, B=B)\n",
    "        pixel_values = torch.from_numpy(np.stack(video_frames, axis=0).transpose(0, 3, 1, 2))\n",
    "        event_voxel_bin = torch.from_numpy(event_voxel_bin.transpose(2, 0, 1)).unsqueeze(0).repeat(len(video_frames), 1, 1, 1)\n",
    "\n",
    "        return pixel_values, event_voxel_bin\n",
    "\n",
    "    def get_batch(self, idx):\n",
    "        video_name = self.video_names[idx]\n",
    "        video_frame_paths = natsorted(glob(os.path.join(self.video_data_dir, video_name, 'events','*.npz')))\n",
    "        rgb_frame_paths   = natsorted(glob(os.path.join(self.video_data_dir, video_name, 'images','*.png')))\n",
    "\n",
    "        start_idx = np.random.randint(0, len(video_frame_paths)-self.sample_frames+1)\n",
    "        video_frame_paths = video_frame_paths[start_idx:start_idx+self.sample_frames]\n",
    "        rgb_frame_paths   = rgb_frame_paths[start_idx:start_idx+self.sample_frames]\n",
    "\n",
    "        return self.accumulate_events(video_frame_paths, rgb_frame_paths, B=6)\n",
    "\n",
    "    def crop_center_patch(self, pixel_values, event_voxel_bin, crop_h=512, crop_w=512, random_crop=False):\n",
    "        H, W = pixel_values.shape[2:4]\n",
    "        if random_crop:\n",
    "            start_h = random.randint(0, H - crop_h)\n",
    "            start_w = random.randint(0, W - crop_w)\n",
    "        else:\n",
    "            start_h = H//2 - crop_h//2\n",
    "            start_w = W//2 - crop_w//2\n",
    "        return (pixel_values[:, :, start_h:start_h+crop_h, start_w:start_w+crop_w],\n",
    "                event_voxel_bin[:, :, start_h:start_h+crop_h, start_w:start_w+crop_w])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        while True:\n",
    "            try:\n",
    "                pixel_values, event_voxel_bin = self.get_batch(idx)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                idx = random.randint(0, self.length-1)\n",
    "\n",
    "        pixel_values = self.pixel_transforms(pixel_values)\n",
    "        event_voxel_bin = event_voxel_bin  # events may not need normalization\n",
    "\n",
    "        pixel_values, event_voxel_bin = self.crop_center_patch(pixel_values, event_voxel_bin, random_crop=True)\n",
    "        conditions = pixel_values[-1]\n",
    "\n",
    "        sample = dict(\n",
    "            pixel_values=pixel_values,       # (T, 3, H, W)\n",
    "            event_voxel_bin=event_voxel_bin, # (T, 6, H, W)\n",
    "            conditions=conditions            # last RGB frame\n",
    "        )\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d06a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset...\n",
      "skip/sample_frames/stride: 1 50 1\n",
      "✅ Dataset initialized successfully. Found 47 videos in '/data/venkateswara_lab/frame_interpollation/bs_ergb/3_TRAINING/'.\n",
      "\n",
      "--- Testing __getitem__ ---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from types import SimpleNamespace  # Used to mock the 'args' object\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Import your StableVideoDataset class here ---\n",
    "# from your_dataset_file import StableVideoDataset\n",
    "\n",
    "# 1. Create a mock 'args' object\n",
    "mock_args = SimpleNamespace(event_filter=\"great_filter\")\n",
    "\n",
    "# 2. Define your data directory\n",
    "data_directory = \"/data/venkateswara_lab/frame_interpollation/bs_ergb/3_TRAINING/\"\n",
    "\n",
    "# 3. Create an instance of the dataset\n",
    "print(\"Initializing dataset...\")\n",
    "try:\n",
    "    dataset = StableVideoDataset(\n",
    "        args=mock_args,\n",
    "        video_data_dir=data_directory,\n",
    "        frame_height=576,  # ✅ corrected name\n",
    "        frame_width=1024,\n",
    "        num_frames=5,\n",
    "        skip_sampling_rate=1\n",
    "    )\n",
    "    print(f\"✅ Dataset initialized successfully. Found {len(dataset)} videos in '{data_directory}'.\")\n",
    "\n",
    "    # --- Optional: Test loading the first item ---\n",
    "    print(\"\\n--- Testing __getitem__ ---\")\n",
    "    sample = dataset[0]\n",
    "    print(\"✅ Successfully loaded sample 0.\")\n",
    "    print(f\"  pixel_values shape: {sample['pixel_values'].shape}\")      # (T, 3, H, W)\n",
    "    print(f\"  event_voxel_bin shape: {sample['event_voxel_bin'].shape}\")  # (T, 6, H, W)\n",
    "\n",
    "    # Display a few tensor values up to 4 decimals\n",
    "    print(\"\\n--- Sample Tensor Values ---\")\n",
    "    print(\"pixel_values[:,:,:4,:4]:\\n\", torch.round(sample['pixel_values'][0, :, :4, :4] * 10000) / 10000)\n",
    "    print(\"event_voxel_bin[:,:,:4,:4]:\\n\", torch.round(sample['event_voxel_bin'][0, 0, :4, :4] * 10000) / 10000)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: Data directory not found at '{data_directory}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred during dataset initialization or testing:\\n{e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdead375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eae6126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import copy\n",
    "from diffusers.utils import load_image, export_to_video\n",
    "from diffusers import UNetSpatioTemporalConditionModel\n",
    "from custom_diffusers.pipelines.pipeline_frame_interpolation_with_noise_injection import FrameInterpolationWithNoiseInjectionPipeline\n",
    "from custom_diffusers.pipelines.evs_pipeline_frame_interpolation_with_noise_injection_color import EVSFrameInterpolationWithNoiseInjectionPipeline\n",
    "from custom_diffusers.schedulers.scheduling_euler_discrete import EulerDiscreteScheduler\n",
    "from attn_ctrl.attention_control import (AttentionStore, \n",
    "                                         register_temporal_self_attention_control, \n",
    "                                         register_temporal_self_attention_flip_control,\n",
    ")\n",
    "from dataset.stable_video_dataset import StableVideoDataset,StableVideoTestDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from einops import rearrange\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "\n",
    "def tensor_to_pillow(tensor, save_path):\n",
    "    # Squeeze the batch dimension if exists and convert to numpy\n",
    "    # print(\"val:\",torch.max(tensor),torch.min(tensor))\n",
    "    image_data = tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()/2.0 + 0.5\n",
    "    image_data = image_data /np.max(image_data) * 255\n",
    "    image_data = image_data.astype(\"uint8\")\n",
    "    # Create a PIL image\n",
    "    pil_image = Image.fromarray(image_data)\n",
    "    # Save the image\n",
    "    pil_image.save(save_path)\n",
    "    return pil_image\n",
    "\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    noise_scheduler = EulerDiscreteScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "    # pipe = FrameInterpolationWithNoiseInjectionPipeline.from_pretrained(\n",
    "    #     args.pretrained_model_name_or_path, \n",
    "    #     scheduler=noise_scheduler,\n",
    "    #     variant=\"fp16\",\n",
    "    #     torch_dtype=torch.float16, \n",
    "    # )\n",
    "    pipe = EVSFrameInterpolationWithNoiseInjectionPipeline.from_pretrained(\n",
    "        args.pretrained_model_name_or_path, \n",
    "        # args.checkpoint_dir,\n",
    "        scheduler=noise_scheduler,\n",
    "        variant=\"fp16\",\n",
    "        torch_dtype=torch.float16, \n",
    "    )\n",
    "    \n",
    "    \n",
    "    # ref_unet = pipe.ori_unet\n",
    "    \n",
    "    # state_dict = pipe.unet.state_dict()\n",
    "    # # computing delta w\n",
    "    finetuned_unet = UNetSpatioTemporalConditionModel.from_pretrained(\n",
    "        args.checkpoint_dir,\n",
    "        subfolder=\"unet\",\n",
    "        torch_dtype=torch.float16,\n",
    "    ) \n",
    "    # # assert finetuned_unet.config.num_frames==14\n",
    "    # ori_unet = UNetSpatioTemporalConditionModel.from_pretrained(\n",
    "    #     \"/mnt/workspace/zhangziran/DiffEVS/svd_keyframe_interpolation-main/checkpoints/stable-video-diffusion-img2vid\",\n",
    "    #     subfolder=\"unet\",\n",
    "    #     variant='fp16',\n",
    "    #     torch_dtype=torch.float16,\n",
    "    # )\n",
    "\n",
    "    # # print(\"-----\"*10)\n",
    "\n",
    "    finetuned_state_dict = finetuned_unet.state_dict()\n",
    "    # ori_state_dict = ori_unet.state_dict()\n",
    "    # for name, param in finetuned_state_dict.items():\n",
    "    #     if 'temporal_transformer_blocks.0.attn1.to_v' in name or \"temporal_transformer_blocks.0.attn1.to_out.0\" in name:\n",
    "    #         delta_w = param - ori_state_dict[name]\n",
    "    #         state_dict[name] = state_dict[name] + delta_w\n",
    "    # pipe.unet.load_state_dict(state_dict)\n",
    "    \n",
    "    pipe.unet.load_state_dict(finetuned_state_dict)\n",
    "\n",
    "    # # controller_ref= AttentionStore()\n",
    "    # # register_temporal_self_attention_control(ref_unet, controller_ref)\n",
    "\n",
    "    # # controller = AttentionStore()\n",
    "    # # register_temporal_self_attention_flip_control(pipe.unet, controller, controller_ref)\n",
    "    \n",
    "    del finetuned_unet\n",
    "    # del ori_unet\n",
    "\n",
    "    pipe = pipe.to(args.device)\n",
    "\n",
    "\n",
    "    print(\"-----\"*10)\n",
    "    \n",
    "    # run inference\n",
    "    generator = torch.Generator(device=args.device)\n",
    "    if args.seed is not None:\n",
    "        generator = generator.manual_seed(args.seed)\n",
    "        \n",
    "    dataset = StableVideoTestDataset(args,args.frames_dirs,num_frames=pipe.unet.config.num_frames,skip_sampling_rate=args.skip_sampling_rate)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "    print(\"-----\"*10)\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        print(i)\n",
    "    \n",
    "        evs = batch[\"event_voxel_bin\"]\n",
    "        frame2 = batch[\"conditions\"]\n",
    "        frame1 = batch[\"pixel_values\"][:, 0]\n",
    "        save_name = batch[\"save_name\"]\n",
    "        print(save_name)\n",
    "        print(frame1.shape,frame2.shape,evs.shape)\n",
    "        \n",
    "        \n",
    "        # frame11 = load_image(args.frame1_path)\n",
    "        # # frame1 = frame1.resize((854, 640))\n",
    "        # frame11 = frame11.resize((1024, 576))\n",
    "        \n",
    "\n",
    "        # frame22 = load_image(args.frame2_path)\n",
    "        # frame22 = frame22.resize((1024, 576))\n",
    "        # # # print(\"************\"*20,frame11.shape,np.mean(frame11))\n",
    "        # from torchvision import transforms\n",
    "        # print(\"************\" * 20, (tensor := transforms.ToTensor()(frame11)).shape, tensor.mean(),frame2.shape)\n",
    "        \n",
    "        # frame2 = frame2.resize((854, 640))\n",
    "        # frame2 = frame2.resize((1024, 576))\n",
    "\n",
    "        frames = pipe(image1=frame1, image2=frame2, evs=evs, height=frame1.shape[-2], width=frame1.shape[-1],\n",
    "                    num_inference_steps=args.num_inference_steps, \n",
    "                    generator=generator,\n",
    "                    weighted_average=args.weighted_average,\n",
    "                    noise_injection_steps=args.noise_injection_steps,\n",
    "                    noise_injection_ratio= args.noise_injection_ratio,\n",
    "        ).frames[0]\n",
    "        save_path = args.out_path\n",
    "        save_path = save_path.replace(\"example\",save_name[0])\n",
    "        \n",
    "        if save_path.endswith('.gif'):\n",
    "            frames[0].save(save_path, save_all=True, append_images=frames[1:], duration=142, loop=0)\n",
    "        else:\n",
    "            export_to_video(frames, save_path, fps=7)\n",
    "        \n",
    "        from eval_function import calculate_metrics\n",
    "        # 假设要保存txt结果\n",
    "        txt_path = save_path.replace(\".gif\", \"_metrics.txt\")\n",
    "        with open(txt_path, \"w\") as f:  # 使用'w'模式创建新文件\n",
    "            f.write(f\"Processing: {save_path}\\n\")\n",
    "            \n",
    "            # 创建用于存储各种指标值的字典\n",
    "            metrics_values = {\n",
    "                'lpips': [],\n",
    "                'ssim': [],\n",
    "                'psnr': [],\n",
    "                'maniqa': [],\n",
    "                'musiq': [],\n",
    "                'liqe': []\n",
    "            }\n",
    "\n",
    "            # 遍历帧，排除初始帧和最后一帧进行评价\n",
    "            for i, frame in enumerate(frames):\n",
    "                if i == 0 or i == len(frames) - 1:  # 排除第一帧和最后一帧\n",
    "                    frame.save(save_path.replace(\".gif\", f\"_{i+1}s.png\"))\n",
    "                    frame_tensor = batch[\"pixel_values\"][:, i]\n",
    "                    frame_pillow = tensor_to_pillow(frame_tensor, save_path.replace(\".gif\", f\"_{i+1}.png\"))\n",
    "                    print(save_path.replace(\".gif\", f\"_{i+1}.png\"), len(frames))\n",
    "                    continue\n",
    "                \n",
    "                frame.save(save_path.replace(\".gif\", f\"_{i+1}s.png\"))\n",
    "                frame_tensor = batch[\"pixel_values\"][:, i]\n",
    "                frame_pillow = tensor_to_pillow(frame_tensor, save_path.replace(\".gif\", f\"_{i+1}.png\"))\n",
    "                print(save_path.replace(\".gif\", f\"_{i+1}.png\"), len(frames))\n",
    "                \n",
    "                # 获取frame和frame_tensor一一对应的图像并计算评价指标\n",
    "                image1 = np.array(frame)  # frame 是 PIL 图像，转换为 numpy 数组\n",
    "                image2 = np.array(frame_pillow) #frame_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()  # 转换 tensor 为 numpy 数组\n",
    "                \n",
    "                print(image1.shape, image2.shape, np.min(image1), np.max(image1), np.min(image2), np.max(image2))\n",
    "\n",
    "                # 计算所有指标\n",
    "                results = calculate_metrics(\n",
    "                    image1, image2, \n",
    "                    loss_dict_lpips={'as_loss': False, 'weight': 1.0},\n",
    "                    loss_dict_ssim={'weight': 1.0}, \n",
    "                    loss_dict_psnr={'as_loss': False, 'weight': 1.0},\n",
    "                    loss_dict_maniqa={'as_loss': False, 'weight': 1.0},\n",
    "                    loss_dict_musiq={'as_loss': False, 'weight': 1.0},\n",
    "                    loss_dict_liqe={'as_loss': False, 'weight': 1.0}\n",
    "                )\n",
    "\n",
    "                # 将当前帧的结果追加到对应的列表中\n",
    "                for metric_name, value in results.items():\n",
    "                    if value is not None:\n",
    "                        metrics_values[metric_name].append(value.cpu().numpy())\n",
    "\n",
    "            # 计算平均指标\n",
    "            avg_metrics = {}\n",
    "            for metric_name, values in metrics_values.items():\n",
    "                if values:  # 确保列表不为空\n",
    "                    avg_metrics[metric_name] = np.mean(values)\n",
    "                else:\n",
    "                    avg_metrics[metric_name] = None\n",
    "\n",
    "            # # 写入平均指标到txt\n",
    "            # f.write(\"Average Metrics:\\n\")\n",
    "            # for metric_name, avg_value in avg_metrics.items():\n",
    "            #     if avg_value is not None:\n",
    "            #         f.write(f\"Average {metric_name.upper()}: {avg_value:.6f}\\n\")\n",
    "\n",
    "            # 写入平均指标到txt（所有指标在一行中）\n",
    "            f.write(\"Average Metrics: \")\n",
    "            metric_strings = []\n",
    "            for metric_name, avg_value in avg_metrics.items():\n",
    "                if avg_value is not None:\n",
    "                    metric_strings.append(f\"{metric_name.upper()}: {avg_value:.6f}\")\n",
    "            f.write(\", \".join(metric_strings) + \"\\n\\n\")  # 将所有指标用逗号分隔写在一行，并添加空行\n",
    "            \n",
    "            f.write(\"\\n\")  # 在每个视频的结果后加一个空行\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--pretrained_model_name_or_path\", type=str, default=\"stabilityai/stable-video-diffusion-img2vid-xt\")\n",
    "    parser.add_argument(\"--checkpoint_dir\", type=str, required=True)\n",
    "    # parser.add_argument('--frame1_path', type=str, required=True)\n",
    "    # parser.add_argument('--frame2_path', type=str, required=True)\n",
    "    parser.add_argument('--out_path', type=str, required=True)\n",
    "    parser.add_argument('--seed', type=int, default=42)\n",
    "    parser.add_argument('--num_inference_steps', type=int, default=50)\n",
    "    parser.add_argument('--weighted_average', action='store_true')\n",
    "    parser.add_argument('--noise_injection_steps', type=int, default=0)\n",
    "    parser.add_argument('--noise_injection_ratio', type=float, default=0.5)\n",
    "    parser.add_argument('--device', type=str, default='cuda:0')\n",
    "    parser.add_argument('--frames_dirs', type=str, default='cuda:0')\n",
    "    parser.add_argument('--event_filter', type=str, default=None)\n",
    "    parser.add_argument('--skip_sampling_rate', type=int, default=1)\n",
    "    args = parser.parse_args()\n",
    "    out_dir = os.path.dirname(args.out_path)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32e7cdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from data import StereoEventDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class StereoEventTestDataset(StereoEventDataset):\n",
    "    def __init__(self, video_data_dir, frame_height=375, frame_width=375):\n",
    "        super().__init__(video_data_dir, frame_height, frame_width)\n",
    "        self.video_names = [self.video_names[0]]\n",
    "        self.length = 1  # Only one video for inference\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_name = self.video_names[idx]\n",
    "        paths = self._get_paths(video_name)\n",
    "\n",
    "        left_rgb = self._load_rgb(paths['left']['rgb'])\n",
    "        left_event = self._load_events(paths['left']['event'])\n",
    "        right_rgb = self._load_rgb(paths['right']['rgb'])\n",
    "        right_event = self._load_events(paths['right']['event'])\n",
    "\n",
    "        # Apply transforms\n",
    "        def apply_transform_to_sequence(sequence_tensor, transform_fn):\n",
    "            if sequence_tensor.ndim == 3:\n",
    "                return transform_fn(sequence_tensor)\n",
    "            transformed_frames = [transform_fn(sequence_tensor[t]) for t in range(sequence_tensor.shape[0])]\n",
    "            return torch.stack(transformed_frames, dim=0)\n",
    "\n",
    "        left_rgb = apply_transform_to_sequence(left_rgb, self.transform_rgb)\n",
    "        right_rgb = apply_transform_to_sequence(right_rgb, self.transform_rgb)\n",
    "        left_event = apply_transform_to_sequence(left_event, self.transforms_evs)\n",
    "        right_event = apply_transform_to_sequence(right_event, self.transforms_evs)\n",
    "\n",
    "        # Use center crop (no random cropping for inference)\n",
    "        left_pixel_values, left_events = self.crop_center_patch(left_rgb, left_event, random_crop=False)\n",
    "        right_pixel_values, right_events = self.crop_center_patch(right_rgb, right_event, random_crop=False)\n",
    "\n",
    "        return dict(\n",
    "            left=dict(pixel_values=left_pixel_values, events=left_events),\n",
    "            right=dict(pixel_values=right_pixel_values, events=right_events),\n",
    "            video_name=video_name)\n",
    "    \n",
    "dataset = StereoEventTestDataset(\"/data/venkateswara_lab/frame_interpollation/data\")\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "print(\"-----\"*10)\n",
    "# for i, batch in enumerate(dataloader):\n",
    "#     print(i)\n",
    "\n",
    "#     evs = batch[\"event_voxel_bin\"]\n",
    "#     frame2 = batch[\"conditions\"][-1]\n",
    "#     frame1 = batch[\"pixel_values\"][:, 0]\n",
    "#     print(frame1.shape,frame2.shape,evs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "435f5c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Batch 0:\n",
      "  Video name: ['stereo_vkitti200000000']\n",
      "  Frame1 shape: torch.Size([1, 3, 576, 1024])\n",
      "  Frame2 shape: torch.Size([1, 3, 576, 1024])\n",
      "  Events shape: torch.Size([1, 17, 6, 375, 375])\n"
     ]
    }
   ],
   "source": [
    "from data import StereoEventDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class StereoEventTestDataset(StereoEventDataset):\n",
    "    def __init__(self, video_data_dir, frame_height=375, frame_width=375):\n",
    "        super().__init__(video_data_dir, frame_height, frame_width)\n",
    "        self.video_names = [self.video_names[0]]  # take only first video\n",
    "        self.length = 1\n",
    "        frame_height = 576\n",
    "        frame_width = 1024\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_name = self.video_names[idx]\n",
    "        paths = self._get_paths(video_name)\n",
    "        resize_transform = T.Compose([\n",
    "    T.Resize((frame_height, frame_width)),  # height, width\n",
    "])\n",
    "\n",
    "        # Load left and right data\n",
    "        left_rgb = self._load_rgb(paths['left']['rgb'])\n",
    "        left_event = self._load_events(paths['left']['event'])\n",
    "        right_rgb = self._load_rgb(paths['right']['rgb'])\n",
    "        right_event = self._load_events(paths['right']['event'])\n",
    "\n",
    "        # Helper for frame-wise transform\n",
    "        def apply_transform_to_sequence(sequence_tensor, transform_fn):\n",
    "            if sequence_tensor.ndim == 3:\n",
    "                return transform_fn(sequence_tensor)\n",
    "            transformed_frames = [transform_fn(sequence_tensor[t]) for t in range(sequence_tensor.shape[0])]\n",
    "            return torch.stack(transformed_frames, dim=0)\n",
    "\n",
    "        left_rgb = apply_transform_to_sequence(left_rgb, self.transform_rgb)\n",
    "        right_rgb = apply_transform_to_sequence(right_rgb, self.transform_rgb)\n",
    "        left_event = apply_transform_to_sequence(left_event, self.transforms_evs)\n",
    "        right_event = apply_transform_to_sequence(right_event, self.transforms_evs)\n",
    "\n",
    "        # Center crop for inference\n",
    "        left_pixel_values, left_events = self.crop_center_patch(left_rgb, left_event, random_crop=False)\n",
    "        right_pixel_values, right_events = self.crop_center_patch(right_rgb, right_event, random_crop=False)\n",
    "\n",
    "        # Select specific frames/events\n",
    "        frame1 = left_pixel_values[0]             # first frame of left RGB\n",
    "        frame2 = left_pixel_values[-1]            # last frame of left RGB\n",
    "        evs = right_events                        # right events\n",
    "\n",
    "        frame1_resized = resize_transform(frame1.squeeze(0))\n",
    "        frame2_resized = resize_transform(frame2.squeeze(0))\n",
    "        return dict(\n",
    "            frame1=frame1_resized,\n",
    "            frame2=frame2_resized,\n",
    "            evs=evs,\n",
    "            video_name=video_name\n",
    "        )\n",
    "\n",
    "\n",
    "frame_height = 576\n",
    "frame_width = 1024\n",
    "\n",
    "# Define resize transform\n",
    "resize_transform = T.Compose([\n",
    "    T.Resize((frame_height, frame_width)),  # height, width\n",
    "])\n",
    "\n",
    "dataset = StereoEventTestDataset(\"/data/venkateswara_lab/frame_interpollation/data\")\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "print(\"-----\" * 10)\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    frame1 = batch['frame1']  # Shape: [B, C, H, W]\n",
    "    frame2 = batch['frame2']  # Shape: [B, C, H, W]\n",
    "    evs = batch['evs']\n",
    "    video_name = batch['video_name']\n",
    "    \n",
    "    print(f\"Batch {i}:\")\n",
    "    print(f\"  Video name: {video_name}\")\n",
    "    print(f\"  Frame1 shape: {frame1.shape}\")\n",
    "    print(f\"  Frame2 shape: {frame2.shape}\")\n",
    "    print(f\"  Events shape: {evs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b74fa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 5/5 [00:00<00:00,  9.42it/s]\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Video 0: ['stereo_vkitti200000000']\n",
      "<class 'torch.Tensor'>\n",
      "Frame1 shape: torch.Size([1, 3, 576, 1024])\n",
      "Frame2 shape: torch.Size([1, 3, 576, 1024])\n",
      "Events shape: torch.Size([1, 17, 6, 375, 375])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "zeros_like(): argument 'input' (position 1) must be Tensor, not AutoencoderKLOutput",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 130\u001b[39m\n\u001b[32m    128\u001b[39m out_dir = os.path.dirname(args.out_path)\n\u001b[32m    129\u001b[39m os.makedirs(out_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFrame2 shape:\u001b[39m\u001b[33m\"\u001b[39m, batch[\u001b[33m\"\u001b[39m\u001b[33mframe2\u001b[39m\u001b[33m\"\u001b[39m].shape)\n\u001b[32m     98\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEvents shape:\u001b[39m\u001b[33m\"\u001b[39m, batch[\u001b[33m\"\u001b[39m\u001b[33mevs\u001b[39m\u001b[33m\"\u001b[39m].shape)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m frames = \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mframe1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mframe1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mevs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mframe1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mframe1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m            \u001b[49m\u001b[43mweighted_average\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweighted_average\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnoise_injection_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnoise_injection_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnoise_injection_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnoise_injection_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.frames[\u001b[32m0\u001b[39m]\n\u001b[32m    107\u001b[39m save_path = args.out_path\n\u001b[32m    109\u001b[39m export_to_video(frames, save_path, fps=\u001b[32m7\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/venkateswara_lab/miniconda3_new/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/venkateswara_lab/frame_interpollation/code/custom_diffusers/pipelines/evs_pipeline_frame_interpolation_with_noise_injection_color.py:556\u001b[39m, in \u001b[36mEVSFrameInterpolationWithNoiseInjectionPipeline.__call__\u001b[39m\u001b[34m(self, image1, image2, evs, height, width, num_frames, num_inference_steps, min_guidance_scale, max_guidance_scale, fps, motion_bucket_id, noise_aug_strength, decode_chunk_size, num_videos_per_prompt, generator, latents, output_type, callback_on_step_end, callback_on_step_end_tensor_inputs, weighted_average, noise_injection_steps, noise_injection_ratio, return_dict)\u001b[39m\n\u001b[32m    551\u001b[39m     \u001b[38;5;28mself\u001b[39m.vae.to(dtype=torch.float32)\n\u001b[32m    554\u001b[39m \u001b[38;5;66;03m# Repeat the image latents for each frame so we can concatenate them with the noise\u001b[39;00m\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# image_latents [batch, channels, height, width] ->[batch, num_frames, channels, height, width]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m image1_latent = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode_vae_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_videos_per_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_classifier_free_guidance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m image1_latent = image1_latent.to(image1_embeddings.dtype)\n\u001b[32m    558\u001b[39m image1_latents = image1_latent.unsqueeze(\u001b[32m1\u001b[39m).repeat(\u001b[32m1\u001b[39m, num_frames, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/venkateswara_lab/frame_interpollation/code/custom_diffusers/pipelines/evs_pipeline_frame_interpolation_with_noise_injection_color.py:133\u001b[39m, in \u001b[36mEVSFrameInterpolationWithNoiseInjectionPipeline._encode_vae_image\u001b[39m\u001b[34m(self, image, device, num_videos_per_prompt, do_classifier_free_guidance)\u001b[39m\n\u001b[32m    130\u001b[39m image_latents = \u001b[38;5;28mself\u001b[39m.vae.encode(image)\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_classifier_free_guidance:\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     negative_image_latents = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_latents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m     \u001b[38;5;66;03m# For classifier free guidance, we need to do two forward passes.\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# Here we concatenate the unconditional and text embeddings into a single batch\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;66;03m# to avoid doing two forward passes\u001b[39;00m\n\u001b[32m    138\u001b[39m     image_latents = torch.cat([negative_image_latents, image_latents])\n",
      "\u001b[31mTypeError\u001b[39m: zeros_like(): argument 'input' (position 1) must be Tensor, not AutoencoderKLOutput"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import copy\n",
    "from diffusers.utils import load_image, export_to_video\n",
    "from diffusers import UNetSpatioTemporalConditionModel\n",
    "from custom_diffusers.pipelines.pipeline_frame_interpolation_with_noise_injection import FrameInterpolationWithNoiseInjectionPipeline\n",
    "from custom_diffusers.pipelines.evs_pipeline_frame_interpolation_with_noise_injection_color import EVSFrameInterpolationWithNoiseInjectionPipeline\n",
    "from custom_diffusers.schedulers.scheduling_euler_discrete import EulerDiscreteScheduler\n",
    "from attn_ctrl.attention_control import (AttentionStore, \n",
    "                                         register_temporal_self_attention_control, \n",
    "                                         register_temporal_self_attention_flip_control)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from einops import rearrange\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "\n",
    "def tensor_to_pillow(tensor, save_path):\n",
    "    image_data = tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()/2.0 + 0.5\n",
    "    image_data = image_data /np.max(image_data) * 255\n",
    "    image_data = image_data.astype(\"uint8\")\n",
    "    pil_image = Image.fromarray(image_data)\n",
    "    pil_image.save(save_path)\n",
    "    return pil_image\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    noise_scheduler = EulerDiscreteScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "    pipe = EVSFrameInterpolationWithNoiseInjectionPipeline.from_pretrained(\n",
    "        args.pretrained_model_name_or_path, \n",
    "        # args.checkpoint_dir,\n",
    "        scheduler=noise_scheduler,\n",
    "        variant=\"fp16\",\n",
    "        torch_dtype=torch.float16, \n",
    "    )\n",
    "    \n",
    "    \n",
    "    # ref_unet = pipe.ori_unet\n",
    "    \n",
    "    # state_dict = pipe.unet.state_dict()\n",
    "    # # computing delta w\n",
    "    finetuned_unet = UNetSpatioTemporalConditionModel.from_pretrained(\n",
    "        args.checkpoint_dir,\n",
    "        subfolder=\"unet\",\n",
    "        torch_dtype=torch.float16,\n",
    "    ) \n",
    "    # # assert finetuned_unet.config.num_frames==14\n",
    "    # ori_unet = UNetSpatioTemporalConditionModel.from_pretrained(\n",
    "    #     \"/mnt/workspace/zhangziran/DiffEVS/svd_keyframe_interpolation-main/checkpoints/stable-video-diffusion-img2vid\",\n",
    "    #     subfolder=\"unet\",\n",
    "    #     variant='fp16',\n",
    "    #     torch_dtype=torch.float16,\n",
    "    # )\n",
    "\n",
    "    # # print(\"-----\"*10)\n",
    "\n",
    "    finetuned_state_dict = finetuned_unet.state_dict()\n",
    "    # ori_state_dict = ori_unet.state_dict()\n",
    "    # for name, param in finetuned_state_dict.items():\n",
    "    #     if 'temporal_transformer_blocks.0.attn1.to_v' in name or \"temporal_transformer_blocks.0.attn1.to_out.0\" in name:\n",
    "    #         delta_w = param - ori_state_dict[name]\n",
    "    #         state_dict[name] = state_dict[name] + delta_w\n",
    "    # pipe.unet.load_state_dict(state_dict)\n",
    "    \n",
    "    pipe.unet.load_state_dict(finetuned_state_dict)\n",
    "\n",
    "    # # controller_ref= AttentionStore()\n",
    "    # # register_temporal_self_attention_control(ref_unet, controller_ref)\n",
    "\n",
    "    # # controller = AttentionStore()\n",
    "    # # register_temporal_self_attention_flip_control(pipe.unet, controller, controller_ref)\n",
    "    \n",
    "    del finetuned_unet\n",
    "    # del ori_unet\n",
    "\n",
    "    pipe = pipe.to(args.device)\n",
    "    print(\"-----\"*10)\n",
    "    generator = torch.Generator(device=args.device)\n",
    "    if args.seed is not None:\n",
    "        generator = generator.manual_seed(args.seed)\n",
    "        \n",
    "\n",
    "    dataset = StereoEventTestDataset(\"/data/venkateswara_lab/frame_interpollation/data\")\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "    i=1\n",
    "    print(\"-----\" * 10)\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        print(f\"Video {i}: {batch['video_name']}\")\n",
    "        print(type(batch[\"frame1\"]))\n",
    "        print(\"Frame1 shape:\", batch[\"frame1\"].shape)\n",
    "        print(\"Frame2 shape:\", batch[\"frame2\"].shape)\n",
    "        print(\"Events shape:\", batch[\"evs\"].shape)\n",
    "        \n",
    "        frames = pipe(image1=batch[\"frame1\"], image2=batch[\"frame1\"], evs=batch[\"evs\"], height=batch[\"frame1\"].shape[-2], width=batch[\"frame1\"].shape[-1],\n",
    "                    num_inference_steps=args.num_inference_steps, \n",
    "                    generator=generator,\n",
    "                    weighted_average=args.weighted_average,\n",
    "                    noise_injection_steps=args.noise_injection_steps,\n",
    "                    noise_injection_ratio= args.noise_injection_ratio,\n",
    "        ).frames[0]\n",
    "        save_path = args.out_path\n",
    "\n",
    "        export_to_video(frames, save_path, fps=7)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--pretrained_model_name_or_path\", type=str, default=\"stabilityai/stable-video-diffusion-img2vid\")\n",
    "    parser.add_argument(\"--checkpoint_dir\", type=str, default ='/data/venkateswara_lab/frame_interpollation/trained_models_full_recent/checkpoint-9500')\n",
    "    # parser.add_argument('--frame1_path', type=str, required=True)\n",
    "    # parser.add_argument('--frame2_path', type=str, required=True)\n",
    "    parser.add_argument('--out_path', type=str, default=\"/data/venkateswara_lab/frame_interpollation/code/examples\")\n",
    "    parser.add_argument('--seed', type=int, default=42)\n",
    "    parser.add_argument('--num_inference_steps', type=int, default=50)\n",
    "    parser.add_argument('--weighted_average', action='store_true')\n",
    "    parser.add_argument('--noise_injection_steps', type=int, default=0)\n",
    "    parser.add_argument('--noise_injection_ratio', type=float, default=0.5)\n",
    "    parser.add_argument('--device', type=str, default='cuda:0')\n",
    "    parser.add_argument('--frames_dirs', type=str, default='cuda:0')\n",
    "    parser.add_argument('--event_filter', type=str, default=None)\n",
    "    parser.add_argument('--skip_sampling_rate', type=int, default=1)\n",
    "    args = parser.parse_args()\n",
    "    out_dir = os.path.dirname(args.out_path)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b0542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import copy\n",
    "from diffusers.utils import load_image, export_to_video\n",
    "from diffusers import UNetSpatioTemporalConditionModel\n",
    "from custom_diffusers.pipelines.pipeline_frame_interpolation_with_noise_injection import FrameInterpolationWithNoiseInjectionPipeline\n",
    "from custom_diffusers.pipelines.evs_pipeline_frame_interpolation_with_noise_injection_color import EVSFrameInterpolationWithNoiseInjectionPipeline\n",
    "from custom_diffusers.schedulers.scheduling_euler_discrete import EulerDiscreteScheduler\n",
    "from attn_ctrl.attention_control import (AttentionStore, \n",
    "                                         register_temporal_self_attention_control, \n",
    "                                         register_temporal_self_attention_flip_control)\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from einops import rearrange\n",
    "from accelerate import Accelerator, DistributedDataParallelKwargs\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "\n",
    "class StereoEventTestDataset(Dataset):\n",
    "    \"\"\"Dataset class for stereo event data\"\"\"\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        # Find all video directories\n",
    "        self.video_dirs = sorted(glob.glob(os.path.join(data_dir, \"*\")))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_dirs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_dir = self.video_dirs[idx]\n",
    "        video_name = os.path.basename(video_dir)\n",
    "        \n",
    "        # Load frame1\n",
    "        frame1_path = os.path.join(video_dir, \"frame1.png\")\n",
    "        frame1 = Image.open(frame1_path).convert(\"RGB\")\n",
    "        frame1 = torch.from_numpy(np.array(frame1)).permute(2, 0, 1).float() / 255.0\n",
    "        frame1 = frame1 * 2.0 - 1.0  # Normalize to [-1, 1]\n",
    "        frame1 = frame1.unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Load frame2\n",
    "        frame2_path = os.path.join(video_dir, \"frame2.png\")\n",
    "        frame2 = Image.open(frame2_path).convert(\"RGB\")\n",
    "        frame2 = torch.from_numpy(np.array(frame2)).permute(2, 0, 1).float() / 255.0\n",
    "        frame2 = frame2 * 2.0 - 1.0  # Normalize to [-1, 1]\n",
    "        frame2 = frame2.unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Load events\n",
    "        evs_path = os.path.join(video_dir, \"events.npy\")\n",
    "        evs = np.load(evs_path)\n",
    "        evs = torch.from_numpy(evs).float()\n",
    "        if evs.dim() == 3:\n",
    "            evs = evs.unsqueeze(0)  # Add batch dimension if needed\n",
    "        \n",
    "        return {\n",
    "            \"frame1\": frame1,\n",
    "            \"frame2\": frame2,\n",
    "            \"evs\": evs,\n",
    "            \"video_name\": video_name\n",
    "        }\n",
    "\n",
    "\n",
    "def tensor_to_pillow(tensor, save_path):\n",
    "    \"\"\"Convert tensor to PIL image and save\"\"\"\n",
    "    image_data = tensor.squeeze(0).permute(1, 2, 0).cpu().numpy() / 2.0 + 0.5\n",
    "    image_data = image_data / np.max(image_data) * 255\n",
    "    image_data = image_data.astype(\"uint8\")\n",
    "    pil_image = Image.fromarray(image_data)\n",
    "    pil_image.save(save_path)\n",
    "    return pil_image\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # Initialize Accelerator for multi-GPU\n",
    "    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=\"fp16\",\n",
    "        kwargs_handlers=[ddp_kwargs]\n",
    "    )\n",
    "    \n",
    "    # Only print on main process\n",
    "    if accelerator.is_main_process:\n",
    "        print(f\"Using {accelerator.num_processes} GPUs for inference\")\n",
    "        print(f\"Process index: {accelerator.process_index}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "    # Load scheduler\n",
    "    noise_scheduler = EulerDiscreteScheduler.from_pretrained(\n",
    "        args.pretrained_model_name_or_path, \n",
    "        subfolder=\"scheduler\"\n",
    "    )\n",
    "    \n",
    "    # Load pipeline\n",
    "    if accelerator.is_main_process:\n",
    "        print(\"Loading base pipeline...\")\n",
    "    \n",
    "    pipe = EVSFrameInterpolationWithNoiseInjectionPipeline.from_pretrained(\n",
    "        args.pretrained_model_name_or_path, \n",
    "        scheduler=noise_scheduler,\n",
    "        variant=\"fp16\",\n",
    "        torch_dtype=torch.float16, \n",
    "    )\n",
    "    \n",
    "    # Load finetuned UNet\n",
    "    if accelerator.is_main_process:\n",
    "        print(f\"Loading finetuned UNet from {args.checkpoint_dir}...\")\n",
    "    \n",
    "    finetuned_unet = UNetSpatioTemporalConditionModel.from_pretrained(\n",
    "        args.checkpoint_dir,\n",
    "        subfolder=\"unet\",\n",
    "        torch_dtype=torch.float16,\n",
    "    ) \n",
    "\n",
    "    finetuned_state_dict = finetuned_unet.state_dict()\n",
    "    pipe.unet.load_state_dict(finetuned_state_dict)\n",
    "    \n",
    "    del finetuned_unet\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        print(\"UNet loaded successfully\")\n",
    "\n",
    "    # Move pipeline components to device\n",
    "    pipe = pipe.to(accelerator.device)\n",
    "    \n",
    "    # Prepare UNet with Accelerator for distributed inference\n",
    "    pipe.unet = accelerator.prepare(pipe.unet)\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        print(\"Model prepared for distributed inference\")\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    # Setup generator\n",
    "    generator = torch.Generator(device=accelerator.device)\n",
    "    if args.seed is not None:\n",
    "        # Different seed for each GPU to add diversity\n",
    "        generator = generator.manual_seed(args.seed + accelerator.process_index)\n",
    "        \n",
    "    # Load dataset\n",
    "    if accelerator.is_main_process:\n",
    "        print(\"Loading dataset...\")\n",
    "    \n",
    "    dataset = StereoEventTestDataset(args.data_dir)\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        print(f\"Found {len(dataset)} videos\")\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=1, \n",
    "        shuffle=False, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Prepare dataloader with Accelerator (splits data across GPUs)\n",
    "    dataloader = accelerator.prepare(dataloader)\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        print(f\"Processing {len(dataset)} samples across {accelerator.num_processes} GPUs\")\n",
    "        print(f\"Each GPU will process approximately {len(dataset) // accelerator.num_processes} samples\")\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    # Process videos\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Calculate global index\n",
    "        global_idx = i * accelerator.num_processes + accelerator.process_index\n",
    "        \n",
    "        video_name = batch['video_name'][0] if isinstance(batch['video_name'], (list, tuple)) else batch['video_name']\n",
    "        \n",
    "        if accelerator.is_main_process or True:  # Print from all processes\n",
    "            print(f\"[GPU {accelerator.process_index}] Processing batch {i} - Video: {video_name}\")\n",
    "            print(f\"[GPU {accelerator.process_index}] Frame1 shape: {batch['frame1'].shape}\")\n",
    "            print(f\"[GPU {accelerator.process_index}] Frame2 shape: {batch['frame2'].shape}\")\n",
    "            print(f\"[GPU {accelerator.process_index}] Events shape: {batch['evs'].shape}\")\n",
    "        \n",
    "        # Move batch to device if not already there\n",
    "        frame1 = batch[\"frame1\"].to(accelerator.device)\n",
    "        frame2 = batch[\"frame2\"].to(accelerator.device)\n",
    "        evs = batch[\"evs\"].to(accelerator.device)\n",
    "        \n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                output = pipe(\n",
    "                    image1=frame1, \n",
    "                    image2=frame2,  # You had frame1 twice in original, assuming this should be frame2\n",
    "                    evs=evs, \n",
    "                    height=frame1.shape[-2], \n",
    "                    width=frame1.shape[-1],\n",
    "                    num_inference_steps=args.num_inference_steps, \n",
    "                    generator=generator,\n",
    "                    weighted_average=args.weighted_average,\n",
    "                    noise_injection_steps=args.noise_injection_steps,\n",
    "                    noise_injection_ratio=args.noise_injection_ratio,\n",
    "                )\n",
    "                frames = output.frames[0]\n",
    "                \n",
    "                # Save output\n",
    "                os.makedirs(args.out_path, exist_ok=True)\n",
    "                save_path = os.path.join(\n",
    "                    args.out_path, \n",
    "                    f\"{video_name}_gpu{accelerator.process_index}.mp4\"\n",
    "                )\n",
    "                \n",
    "                export_to_video(frames, save_path, fps=args.fps)\n",
    "                \n",
    "                print(f\"[GPU {accelerator.process_index}] Saved video to: {save_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"[GPU {accelerator.process_index}] Error processing {video_name}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    # Wait for all processes to finish\n",
    "    accelerator.wait_for_everyone()\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        print(\"=\" * 50)\n",
    "        print(\"All videos processed successfully!\")\n",
    "        print(f\"Output saved to: {args.out_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description=\"Multi-GPU Frame Interpolation Inference\")\n",
    "    \n",
    "    # Model arguments\n",
    "    parser.add_argument(\n",
    "        \"--pretrained_model_name_or_path\", \n",
    "        type=str, \n",
    "        default=\"stabilityai/stable-video-diffusion-img2vid\",\n",
    "        help=\"Path to pretrained model\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--checkpoint_dir\", \n",
    "        type=str, \n",
    "        default='/data/venkateswara_lab/frame_interpollation/trained_models_full_recent/checkpoint-9500',\n",
    "        help=\"Path to finetuned checkpoint\"\n",
    "    )\n",
    "    \n",
    "    # Data arguments\n",
    "    parser.add_argument(\n",
    "        '--data_dir',\n",
    "        type=str,\n",
    "        default=\"/data/venkateswara_lab/frame_interpollation/data\",\n",
    "        help=\"Directory containing input data\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--out_path', \n",
    "        type=str, \n",
    "        default=\"/data/venkateswara_lab/frame_interpollation/code/examples\",\n",
    "        help=\"Output directory for generated videos\"\n",
    "    )\n",
    "    \n",
    "    # Inference arguments\n",
    "    parser.add_argument(\n",
    "        '--seed', \n",
    "        type=int, \n",
    "        default=42,\n",
    "        help=\"Random seed\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_inference_steps', \n",
    "        type=int, \n",
    "        default=50,\n",
    "        help=\"Number of denoising steps\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--fps',\n",
    "        type=int,\n",
    "        default=7,\n",
    "        help=\"Output video FPS\"\n",
    "    )\n",
    "    \n",
    "    # Noise injection arguments\n",
    "    parser.add_argument(\n",
    "        '--weighted_average', \n",
    "        action='store_true',\n",
    "        help=\"Use weighted average\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--noise_injection_steps', \n",
    "        type=int, \n",
    "        default=0,\n",
    "        help=\"Number of noise injection steps\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--noise_injection_ratio', \n",
    "        type=float, \n",
    "        default=0.5,\n",
    "        help=\"Noise injection ratio\"\n",
    "    )\n",
    "    \n",
    "    # Additional arguments\n",
    "    parser.add_argument(\n",
    "        '--event_filter', \n",
    "        type=str, \n",
    "        default=None,\n",
    "        help=\"Event filter type\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--skip_sampling_rate', \n",
    "        type=int, \n",
    "        default=1,\n",
    "        help=\"Skip sampling rate\"\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(args.out_path, exist_ok=True)\n",
    "    \n",
    "    # Run main\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
